import streamlit as st
import sounddevice as sd
import torchaudio
import tempfile
import torch
import re
import cv2
import numpy as np
from pathlib import Path
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

# ----------- Setup -------------
SAMPLE_RATE = 16000
MODEL_ID = "manandey/wav2vec2-large-xlsr-assamese"

# Get the base directory of app.py
BASE_DIR = Path(__file__).resolve().parent

# Dynamically define dataset path
DATASET_PATH = BASE_DIR / "datasets"

@st.cache_resource
def load_model():
    processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)
    model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)
    return processor, model

processor, model = load_model()

# Assamese stopwords and dataset
assamese_stopwords = {
    "à¦•à¦¿à¦¨à§à¦¤à§", "à¦†à¦›à§‡", "à¦à¦Ÿà¦¾", "à¦à¦Ÿà¦¿", "à¦à¦‡à¦Ÿà§‹", "à¦¤à¦¾à¦¤", "à¦¨à§‡à¦•à¦¿", "à¦•à§°à¦¿", "à¦²à¦¾à¦—à§‡", "à¦¤à§‡à¦“à¦",
    "à¦¤à§à¦®à¦¿", "à¦†à¦®à¦¿", "à¦®à¦‡", "à¦†à¦ªà§à¦¨à¦¿", "à¦¨à¦¾à¦‡", "à¦¤à§‹à¦®à¦¾à¦•", "à¦¯à¦¿", "à¦•à§‹à¦¨", "à¦¤à§‡à¦“à¦à¦²à§‹à¦•à§‡",
    "à¦•à§°à¦¾", "à¦•à§°à¦¿à¦›à¦¾", "à¦•à§°à¦¿à¦›à§‡", "à¦ªà¦¾à§°à§‹", "à¦ªà¦¾à§°à¦¿à¦¬", "à¦•'à§°", "à¦†à¦¹à¦¿à¦›à§‡", "à¦¦à¦¿à¦¯à¦¼à¦¾", "à¦—à§ˆ", "à¦¯à¦¾à¦®"
}

dataset_sentences = [
    "à¦…à¦¨à§à¦—à§à§°à¦¹ à¦•à§°à¦¿ à¦†à¦ªà§à¦¨à¦¿ à¦à¦‡à¦Ÿà§‹ à¦ªà§à¦¨à§°à¦¾à¦¬à§ƒà¦¤à§à¦¤à¦¿ à¦•à§°à¦¿à¦¬ à¦ªà¦¾à§°à¦¿à¦¬ à¦¨à§‡à¦•à¦¿", "à¦…à¦¨à§à¦—à§à§°à¦¹ à¦•à§°à¦¿ à¦²à¦¾à¦¹à§‡ à¦²à¦¾à¦¹à§‡ à¦•à¦¥à¦¾ à¦ªà¦¾à¦¤à¦¿à¦¬ à¦ªà¦¾à§°à¦¿à¦¬ à¦¨à§‡à¦•à¦¿",
    "à¦†à¦œà¦¿ à¦†à¦ªà§à¦¨à¦¿ à¦†à¦œà§°à¦¿ à¦†à¦›à§‡ à¦¨à¦¿à¦•à¦¿", "à¦†à¦ªà§à¦¨à¦¿ à¦‡à¦®à¦¾à¦¨ à¦¦à¦¯à¦¼à¦¾à¦²à§", "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿ à¦•à§°à¦¿à¦›à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿ à¦•à§°à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿ à¦¬à¦¿à¦šà¦¾à§°à§‡",
    "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿ à¦­à¦¾à¦¬à¦¿à¦›à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿à¦¯à¦¼ à¦•à¦¾à¦¨à§à¦¦à¦¿à¦›à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦•à¦¿à§Ÿ à¦–à¦‚ à¦•à§°à¦¿ï¿½à¦›à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦•à§‹à¦¨",
    "à¦†à¦ªà§à¦¨à¦¿ à¦•à§‹à¦¨à¦–à¦¨ à¦•à¦²à§‡à¦œ_à¦¸à§à¦•à§à¦²à§° à¦¹à§Ÿ", "à¦†à¦ªà§à¦¨à¦¿ à¦•à§°à¦¿à¦¬ à¦ªà¦¾à§°à¦¿à¦¬", "à¦†à¦ªà§à¦¨à¦¿ à¦•â€™à§° à¦ªà§°à¦¾ à¦†à¦¹à¦¿à¦›à§‡", "à¦†à¦ªà§à¦¨à¦¿ à¦–à¦¾à¦²à§‡à¦¨à§‡",
    "à¦†à¦ªà§‹à¦¨à¦¾à¦• à¦•à§‡à¦¨à§‡à¦•à§ˆ à¦¸à¦¹à¦¾à¦¯à¦¼ à¦•à§°à¦¿à¦®", "à¦†à¦ªà§‹à¦¨à¦¾à§° à¦•à§‡à§°à¦¿à¦¯à¦¼à¦¾à§°à§° à¦¬à¦¾à¦¬à§‡ à¦•à¦¿ à¦ªà§°à¦¿à¦•à¦²à§à¦ªà¦¨à¦¾ à¦•à§°à¦¿à¦›à§‡", "à¦†à¦ªà§‹à¦¨à¦¾à§° à¦«à§‹à¦¨ à¦¨à¦®à§à¦¬à§°à¦Ÿà§‹ à¦•à¦¿",
    "à¦†à¦ªà§‹à¦¨à¦¾à§° à¦¬à§Ÿà¦¸ à¦•à¦¿à¦®à¦¾à¦¨", "à¦†à¦®à¦¿ à¦¬à¦¾à¦¹à¦¿à§°à¦²à§ˆ à¦¯à¦¾à¦® à¦¨à§‡à¦•à¦¿", "à¦•à¦¿ à¦•à¦²à¦¾ à¦¤à¦¾à¦•", "à¦•à¦¿ à¦•à§°à¦¿à¦® à¦®à¦‡ à¦¦à§‹à¦®à§‹à¦œà¦¾à¦¤ à¦ªà§°à¦¿à¦›à§‹", "à¦•à¦¿ à¦¹'à¦¬ à¦¬à¦¿à¦šà¦¾à§°à¦¿à¦›à¦¾",
    "à¦•à¦¿ à¦¹â€™à¦²", "à¦•à¦¿à¦¬à¦¾ à¦²à¦¾à¦—à§‡ à¦¨à§‡à¦•à¦¿", "à¦•à¦¿à¦¬à¦¾ à¦²à§à¦•à§à§±à¦¾à¦‡à¦›à¦¾ à¦¨à§‡à¦•à¦¿", "à¦•à§‡à¦¤à¦¿à¦¯à¦¼à¦¾ à¦¯à¦¾à¦¬ à§°à§‡à¦²à¦–à¦¨", "à¦•à§‡à¦¨à§‡à¦•à§ˆ à¦¬à¦¿à¦¶à§à¦¬à¦¾à¦¸ à¦•à§°à¦¿à¦® à¦¤à§‹à¦®à¦¾à¦•",
    "à¦•à§‡à¦¨à§‡à¦•à§ˆ à¦¸à¦¾à¦¹à¦¸ à¦•à§°à¦¿à¦²à§‡", "à¦šà¦¿à¦¨à§à¦¤à¦¾ à¦•à§°à¦¾à§° à¦ªà§à§°à¦¯à¦¼à§‹à¦œà¦¨ à¦¨à¦¾à¦‡ à¦šà¦¿à¦¨à§à¦¤à¦¾ à¦¨à¦•à§°à¦¿à¦¬", "à¦¤à¦¾à¦¤ à¦®à¦‡ à¦†à¦ªà§‹à¦¨à¦¾à¦• à¦¸à¦¹à¦¾à¦¯à¦¼ à¦•à§°à¦¿à¦¬ à¦¨à§‹à§±à¦¾à§°à§‹",
    "à¦¤à§à¦®à¦¿ à¦•à¦¿à¦¯à¦¼ à¦¹à¦¤à¦¾à¦¶ à¦¹à§ˆà¦›à¦¾", "à¦¤à§à¦®à¦¿ à¦¯à¦¿à¦¯à¦¼à§‡ à¦¨à¦•à§°à¦¾ à¦•à¦¿à¦¯à¦¼, à¦®à¦‡ à¦à¦•à§‹ à¦—à§à§°à§à¦¤à§à¦¬ à¦¨à¦¿à¦¦à¦¿à¦“à¦à¥¤", "à¦¤à§‡à¦“à¦ _à¦¤à¦¾à¦‡ à¦®à§‹à§° à¦¬à¦¨à§à¦§à§",
    "à¦¤à§‹à¦®à¦¾à§° à¦²à¦—à¦¤ à¦•à¦¥à¦¾ à¦ªà¦¾à¦¤à¦¿ à¦­à¦¾à¦² à¦•à¦¾à¦—à¦¿à¦²", "à¦¨à¦®à¦¸à§à¦•à¦¾à§°", "à¦¨à¦®à¦¸à§à¦•à¦¾à§°, à¦†à¦ªà§‹à¦¨à¦¾à§° à¦•à¦¿ à¦–à¦¬à§°", "à¦®à¦‡ à¦†à¦ªà§‹à¦¨à¦¾à¦• à¦¸à¦¹à¦¾à¦¯à¦¼ à¦•à§°à¦¿à¦¬ à¦ªà¦¾à§°à§‹ à¦¨à§‡",
    "à¦®à¦‡ à¦à¦•à¦®à¦¤ à¦¨à¦¹à¦¯à¦¼", "à¦®à¦‡ à¦ à¦¿à¦•à§‡à¦‡ à¦†à¦›à§‹à¥¤ à¦§à¦¨à§à¦¯à¦¬à¦¾à¦¦ à¦›à¦¾à§°", "à¦®à§‹à¦• à¦•à§‹à¦¨à§‹à¦¬à¦¾à¦‡ à§°à¦–à¦¾à¦‡ à¦¦à¦¿à¦²à§‡", "à¦®à§‹à§° à¦¤à§‹à¦®à¦¾à¦• à¦­à¦¾à¦² à¦²à¦¾à¦—à§‡_à¦®à¦‡ à¦¤à§‹à¦®à¦¾à¦• à¦­à¦¾à¦² à¦ªà¦¾à¦“à¦",
    "à¦®à§‹à§° à¦¬à¦¾à¦¬à§‡ à¦‡à¦¯à¦¼à¦¾à§° à¦•à§‹à¦¨à§‹ à¦ªà¦¾à§°à§à¦¥à¦•à§à¦¯ à¦¨à¦¾à¦‡", "à¦¯à§‹à§±à¦¾ à¦†à§°à§ à¦¶à§à¦‡ à¦¦à¦¿à¦¯à¦¼à¦¾", "à¦¸à¦à¦šà¦¾ à¦•à¦¥à¦¾ à¦•à§‹à§±à¦¾", "à¦¸à¦¿ à¦•à§‹à¦ à¦¾à¦Ÿà§‹à¦¤ à¦¸à§‹à¦®à¦¾à¦‡ à¦—à§ˆà¦›à§‡", "à¦¸à¦¿ à¦—à§ˆ à¦†à¦›à§‡"
]

# ----------- Functions -------------

def record_audio(duration=5):
    st.info("ğŸ™ï¸ Recording... Speak into your microphone")
    audio = sd.rec(int(duration * SAMPLE_RATE), samplerate=SAMPLE_RATE, channels=1, dtype='float32')
    sd.wait()
    return audio.squeeze()

def transcribe(audio):
    input_values = processor(audio, return_tensors="pt", sampling_rate=SAMPLE_RATE).input_values
    with torch.no_grad():
        logits = model(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0]
    return transcription.strip()

def preprocess(text):
    text = re.sub(r'[^\u0980-\u09FF\s]', '', text)
    return re.sub(r'\s+', ' ', text).strip()

def gloss(text):
    tokens = preprocess(text).split()
    return [t for t in tokens if t not in assamese_stopwords]

def jaccard_similarity(tokens1, tokens2):
    s1, s2 = set(tokens1), set(tokens2)
    if not s1 or not s2:
        return 0
    return len(s1 & s2) / len(s1 | s2)

def find_best_match(input_text):
    input_gloss = gloss(input_text)
    glossed_dataset = [gloss(sent) for sent in dataset_sentences]
    best_score, best_sentence = 0.0, None

    for sent, glossed in zip(dataset_sentences, glossed_dataset):
        score = jaccard_similarity(input_gloss, glossed)
        if score > best_score:
            best_score, best_sentence = score, sent

    if best_score >= 0.2:
        return best_sentence, best_score
    return None, 0.0  # No match found

def get_video_path(sentence):
    return DATASET_PATH / sentence / "à¦‡.mp4"

# ----------- Streamlit UI -------------

st.title("ğŸ§ Assamese Speech to Sign Language")
st.markdown("Speak or type in Assamese, and watch the matching sign language video.")

col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("Input")
    text_input = st.text_area("Enter Assamese text:", height=100)

    if st.button("ğŸ¤ Record"):
        audio_data = record_audio(duration=5)
        transcription = transcribe(audio_data)
        st.success(f"ğŸ“ Transcribed Text: `{transcription}`")
        text_input = transcription  # Use transcribed text

    if text_input:
        best_sentence, score = find_best_match(text_input)
        if best_sentence:
            st.write(f"ğŸ“˜ Matched Sentence: `{best_sentence}` (Similarity: {score:.2f})")
        else:
            st.warning("âš ï¸ No matching sentence found. Please try a different input.")
    else:
        best_sentence = None

with col2:
    st.subheader("Sign Language Video")
    if best_sentence:
        video_path = get_video_path(best_sentence)
        if video_path.exists():
            st.video(str(video_path))
        else:
            st.error(f"âš ï¸ Video not found: `{video_path}`")
    else:
        st.info("No input provided or no match found. Please type or record to see the video.")
